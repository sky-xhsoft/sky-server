# Cloud æ¨¡å—æ–­ç‚¹ç»­ä¼ åŠŸèƒ½åˆ†ææŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ€»ç»“

**åˆ†ææ—¶é—´**: 2026-01-15
**æ¨¡å—**: Cloudï¼ˆäº‘ç›˜ï¼‰
**ç‰ˆæœ¬**: Phase 13+
**ç»“è®º**: âŒ **ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ **

---

## ğŸ” è¯¦ç»†åˆ†æ

### ä¸€ã€å½“å‰å®ç°æƒ…å†µ

#### 1. ä¸Šä¼ å®ç°æ–¹å¼

**Handlerå±‚** (`api/handler/cloud_handler.go:209-262`):
```go
func (h *CloudHandler) UploadFile(c *gin.Context) {
    // è·å–ä¸Šä¼ çš„æ–‡ä»¶
    fileHeader, err := c.FormFile("file")
    if err != nil {
        utils.BadRequest(c, "æœªæ‰¾åˆ°ä¸Šä¼ æ–‡ä»¶")
        return
    }

    // æ‰“å¼€æ–‡ä»¶
    file, err := fileHeader.Open()
    if err != nil {
        utils.InternalError(c, "æ‰“å¼€æ–‡ä»¶å¤±è´¥: " + err.Error())
        return
    }
    defer file.Close()

    // æ„é€ ä¸Šä¼ è¯·æ±‚ï¼ˆä¸€æ¬¡æ€§ä¼ è¾“æ•´ä¸ªæ–‡ä»¶ï¼‰
    uploadReq := &cloud.UploadFileRequest{
        FileName:    fileHeader.Filename,
        FolderID:    folderID,
        FileSize:    fileHeader.Size,
        FileType:    fileHeader.Header.Get("Content-Type"),
        Reader:      file,              // âš ï¸ æ•´ä¸ªæ–‡ä»¶çš„ Reader
        StorageType: "local",
    }

    uploadedFile, err := h.cloudService.UploadFile(c.Request.Context(), uploadReq, userID.(uint))
    // ...
}
```

**Serviceå±‚** (`internal/service/cloud/cloud_service.go:253-314`):
```go
func (s *service) UploadFile(ctx context.Context, req *UploadFileRequest, userID uint) (*entity.CloudFile, error) {
    // 1. æ£€æŸ¥é…é¢
    if err := s.CheckQuota(ctx, userID, req.FileSize); err != nil {
        return nil, err
    }

    // 2. æ„å»ºå­˜å‚¨è·¯å¾„ï¼ˆUUIDï¼‰
    ext := filepath.Ext(req.FileName)
    storageName := fmt.Sprintf("%s%s", uuid.New().String(), ext)
    dateDir := time.Now().Format("2006/01/02")
    storagePath := fmt.Sprintf("cloud/%d/%s/%s", userID, dateDir, storageName)

    // 3. ä¸Šä¼ åˆ°å­˜å‚¨ï¼ˆâš ï¸ ä¸€æ¬¡æ€§å®Œæ•´ä¸Šä¼ ï¼‰
    accessURL, err := s.storage.Upload(ctx, storagePath, req.Reader, req.FileType)
    if err != nil {
        return nil, err
    }

    // 4. åˆ›å»ºæ–‡ä»¶è®°å½•
    file := &entity.CloudFile{
        FileName:    req.FileName,
        StoragePath: storagePath,
        FileSize:    req.FileSize,
        // ...
    }

    if err := s.db.WithContext(ctx).Create(file).Error; err != nil {
        s.storage.Delete(ctx, storagePath) // âš ï¸ å¤±è´¥æ—¶åˆ é™¤ï¼Œä¸ä¿ç•™å·²ä¸Šä¼ çš„éƒ¨åˆ†
        return nil, errors.Wrap(errors.ErrDatabase, "åˆ›å»ºæ–‡ä»¶è®°å½•å¤±è´¥", err)
    }

    return file, nil
}
```

**Storageå±‚** (`internal/pkg/storage/local_storage.go:40-67`):
```go
func (s *LocalStorage) Upload(ctx context.Context, path string, reader io.Reader, contentType string) (string, error) {
    fullPath := filepath.Join(s.basePath, path)

    // åˆ›å»ºæ–‡ä»¶
    file, err := os.Create(fullPath)
    if err != nil {
        return "", errors.Wrap(errors.ErrInternal, "åˆ›å»ºæ–‡ä»¶å¤±è´¥", err)
    }
    defer file.Close()

    // âš ï¸ ä¸€æ¬¡æ€§å†™å…¥æ•´ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ io.Copy
    if _, err := io.Copy(file, reader); err != nil {
        os.Remove(fullPath) // âš ï¸ å¤±è´¥æ—¶åˆ é™¤æ–‡ä»¶ï¼Œä¸ä¿ç•™å·²å†™å…¥çš„éƒ¨åˆ†
        return "", errors.Wrap(errors.ErrInternal, "å†™å…¥æ–‡ä»¶å¤±è´¥", err)
    }

    url := fmt.Sprintf("%s/%s", s.baseURL, path)
    return url, nil
}
```

**OSSå±‚** (`internal/pkg/storage/aliyun_oss.go:54-74`):
```go
func (s *AliyunOSS) Upload(ctx context.Context, path string, reader io.Reader, contentType string) (string, error) {
    options := []oss.Option{
        oss.ContentType(contentType),
    }

    // âš ï¸ ä½¿ç”¨ PutObject ä¸€æ¬¡æ€§ä¸Šä¼ æ•´ä¸ªæ–‡ä»¶
    err := s.bucket.PutObject(path, reader, options...)
    if err != nil {
        return "", errors.Wrap(errors.ErrInternal, "ä¸Šä¼ æ–‡ä»¶åˆ°OSSå¤±è´¥", err)
    }

    url, err := s.GetURL(ctx, path, 0)
    if err != nil {
        return "", err
    }

    return url, nil
}
```

#### 2. ä¸‹è½½å®ç°æ–¹å¼

**Download Handler** (`api/handler/cloud_handler.go:264-304`):
```go
func (h *CloudHandler) DownloadFile(c *gin.Context) {
    reader, fileInfo, err := h.cloudService.DownloadFile(c.Request.Context(), uint(id), userID.(uint))
    if err != nil {
        utils.InternalError(c, "ä¸‹è½½æ–‡ä»¶å¤±è´¥: " + err.Error())
        return
    }
    defer reader.Close()

    // âš ï¸ è®¾ç½®å“åº”å¤´ï¼Œä¸æ”¯æŒ Range è¯·æ±‚
    c.Header("Content-Disposition", "attachment; filename=" + fileInfo.FileName)
    c.Header("Content-Type", fileInfo.FileType)
    c.Header("Content-Length", strconv.FormatInt(fileInfo.FileSize, 10))

    // âš ï¸ æµå¼ä¼ è¾“æ•´ä¸ªæ–‡ä»¶
    if _, err := io.Copy(c.Writer, reader); err != nil {
        utils.InternalError(c, "ä¼ è¾“æ–‡ä»¶å¤±è´¥: " + err.Error())
        return
    }
}
```

**Download Service** (`internal/service/cloud/cloud_service.go:513-537`):
```go
func (s *service) DownloadFile(ctx context.Context, fileID uint, userID uint) (io.ReadCloser, *entity.CloudFile, error) {
    file, err := s.getFileByID(ctx, fileID)
    if err != nil {
        return nil, nil, err
    }

    // æ£€æŸ¥æƒé™
    if file.OwnerID != userID {
        return nil, nil, errors.New(errors.ErrPermissionDenied, "æ— æƒé™ä¸‹è½½æ­¤æ–‡ä»¶")
    }

    // âš ï¸ ä»å­˜å‚¨ä¸­ä¸‹è½½å®Œæ•´æ–‡ä»¶
    reader, err := s.storage.Download(ctx, file.StoragePath)
    if err != nil {
        return nil, nil, err
    }

    // æ›´æ–°ä¸‹è½½æ¬¡æ•°
    s.db.WithContext(ctx).Model(&entity.CloudFile{}).
        Where("ID = ?", fileID).
        Update("DOWNLOAD_COUNT", gorm.Expr("DOWNLOAD_COUNT + 1"))

    return reader, file, nil
}
```

---

### äºŒã€ç¼ºå¤±çš„åŠŸèƒ½

#### âŒ 1. **åˆ†ç‰‡ä¸Šä¼ ï¼ˆMultipart Uploadï¼‰**

**ç°çŠ¶**:
- åªæ”¯æŒå•æ¬¡å®Œæ•´ä¸Šä¼ 
- ä½¿ç”¨ `c.FormFile()` è·å–æ•´ä¸ªæ–‡ä»¶
- æ²¡æœ‰åˆ†ç‰‡ç®¡ç†æœºåˆ¶

**éœ€è¦**:
```go
// éœ€è¦å®ç°çš„æ¥å£
type ChunkUpload struct {
    FileID       string  // æ–‡ä»¶å”¯ä¸€æ ‡è¯†
    ChunkIndex   int     // åˆ†ç‰‡ç´¢å¼•
    TotalChunks  int     // æ€»åˆ†ç‰‡æ•°
    ChunkSize    int64   // åˆ†ç‰‡å¤§å°
    ChunkData    []byte  // åˆ†ç‰‡æ•°æ®
    ChunkMD5     string  // åˆ†ç‰‡MD5
}

// éœ€è¦çš„API
POST /api/v1/cloud/files/multipart/init     // åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ 
POST /api/v1/cloud/files/multipart/upload   // ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
POST /api/v1/cloud/files/multipart/complete // å®Œæˆä¸Šä¼ å¹¶åˆå¹¶
POST /api/v1/cloud/files/multipart/abort    // å–æ¶ˆä¸Šä¼ 
GET  /api/v1/cloud/files/multipart/status   // æŸ¥è¯¢ä¸Šä¼ çŠ¶æ€
```

#### âŒ 2. **æ–­ç‚¹ç»­ä¼ ï¼ˆResume Uploadï¼‰**

**ç°çŠ¶**:
- æ²¡æœ‰ä¸Šä¼ è¿›åº¦è®°å½•
- ä¸Šä¼ å¤±è´¥åå®Œå…¨åˆ é™¤å·²ä¸Šä¼ çš„æ•°æ®
- æ²¡æœ‰åˆ†ç‰‡çŠ¶æ€è·Ÿè¸ª

**éœ€è¦**:
```go
// éœ€è¦çš„æ•°æ®è¡¨
type UploadSession struct {
    ID           uint      // ä¼šè¯ID
    UserID       uint      // ç”¨æˆ·ID
    FileID       string    // æ–‡ä»¶å”¯ä¸€æ ‡è¯†ï¼ˆMD5æˆ–UUIDï¼‰
    FileName     string    // æ–‡ä»¶å
    FileSize     int64     // æ–‡ä»¶æ€»å¤§å°
    ChunkSize    int64     // åˆ†ç‰‡å¤§å°
    TotalChunks  int       // æ€»åˆ†ç‰‡æ•°
    UploadedChunks []int   // å·²ä¸Šä¼ çš„åˆ†ç‰‡ç´¢å¼•åˆ—è¡¨
    Status       string    // çŠ¶æ€: uploading, paused, completed, failed
    ExpireTime   time.Time // è¿‡æœŸæ—¶é—´
    StoragePath  string    // ä¸´æ—¶å­˜å‚¨è·¯å¾„
}

// åˆ†ç‰‡è®°å½•
type ChunkRecord struct {
    SessionID   uint      // ä¼šè¯ID
    ChunkIndex  int       // åˆ†ç‰‡ç´¢å¼•
    ChunkMD5    string    // åˆ†ç‰‡MD5
    ChunkPath   string    // åˆ†ç‰‡å­˜å‚¨è·¯å¾„
    Uploaded    bool      // æ˜¯å¦å·²ä¸Šä¼ 
    UploadTime  time.Time // ä¸Šä¼ æ—¶é—´
}
```

#### âŒ 3. **Range è¯·æ±‚æ”¯æŒ**

**ç°çŠ¶**:
```go
// å½“å‰ä¸‹è½½å®ç°ä¸æ”¯æŒ Range è¯·æ±‚
c.Header("Content-Disposition", "attachment; filename=" + fileInfo.FileName)
c.Header("Content-Type", fileInfo.FileType)
c.Header("Content-Length", strconv.FormatInt(fileInfo.FileSize, 10))
// âš ï¸ ç¼ºå°‘ Accept-Ranges å¤´
// âš ï¸ ä¸å¤„ç† Range è¯·æ±‚å¤´

io.Copy(c.Writer, reader) // âš ï¸ å§‹ç»ˆä¼ è¾“æ•´ä¸ªæ–‡ä»¶
```

**éœ€è¦**:
```go
// éœ€è¦æ”¯æŒ HTTP Range è¯·æ±‚
func (h *CloudHandler) DownloadFile(c *gin.Context) {
    // 1. è§£æ Range è¯·æ±‚å¤´
    rangeHeader := c.GetHeader("Range")

    // 2. è®¾ç½®å“åº”å¤´
    c.Header("Accept-Ranges", "bytes")

    if rangeHeader != "" {
        // 3. è§£æ Rangeï¼ˆå¦‚: bytes=0-1023ï¼‰
        start, end := parseRange(rangeHeader, fileSize)

        // 4. è®¾ç½® 206 Partial Content å“åº”
        c.Status(http.StatusPartialContent)
        c.Header("Content-Range", fmt.Sprintf("bytes %d-%d/%d", start, end, fileSize))
        c.Header("Content-Length", fmt.Sprintf("%d", end-start+1))

        // 5. è¯»å–æŒ‡å®šèŒƒå›´çš„æ•°æ®
        reader.Seek(start, io.SeekStart)
        io.CopyN(c.Writer, reader, end-start+1)
    } else {
        // 6. å®Œæ•´æ–‡ä»¶ä¸‹è½½
        c.Status(http.StatusOK)
        c.Header("Content-Length", fmt.Sprintf("%d", fileSize))
        io.Copy(c.Writer, reader)
    }
}
```

#### âŒ 4. **ç§’ä¼ åŠŸèƒ½ï¼ˆæ–‡ä»¶å»é‡ï¼‰**

**ç°çŠ¶**:
```go
// CloudFile å®ä½“ä¸­æœ‰ MD5 å­—æ®µï¼Œä½†æœªä½¿ç”¨
type CloudFile struct {
    // ...
    MD5 string `gorm:"column:MD5;size:32;index" json:"md5"` // âš ï¸ æœªä½¿ç”¨
    // ...
}
```

**éœ€è¦**:
```go
// ç§’ä¼ åŠŸèƒ½å®ç°
func (s *service) QuickUpload(ctx context.Context, fileMD5 string, fileName string, userID uint) (*entity.CloudFile, error) {
    // 1. æŸ¥è¯¢æ˜¯å¦å­˜åœ¨ç›¸åŒMD5çš„æ–‡ä»¶
    var existingFile entity.CloudFile
    err := s.db.WithContext(ctx).
        Where("MD5 = ? AND IS_ACTIVE = ?", fileMD5, "Y").
        First(&existingFile).Error

    if err == nil {
        // 2. å­˜åœ¨ç›¸åŒæ–‡ä»¶ï¼Œå¤åˆ¶è®°å½•ï¼ˆç§’ä¼ ï¼‰
        newFile := &entity.CloudFile{
            FileName:    fileName,
            StoragePath: existingFile.StoragePath, // å…±äº«å­˜å‚¨è·¯å¾„
            FileSize:    existingFile.FileSize,
            FileType:    existingFile.FileType,
            MD5:         fileMD5,
            OwnerID:     userID,
            // ...
        }

        // 3. åˆ›å»ºæ–°è®°å½•ï¼Œä¸å®é™…ä¸Šä¼ æ–‡ä»¶
        s.db.WithContext(ctx).Create(newFile)

        // 4. æ›´æ–°é…é¢ï¼ˆç©ºé—´å·²å ç”¨ï¼Œåªå¢åŠ æ–‡ä»¶è®¡æ•°ï¼‰
        s.UpdateQuota(ctx, userID, 0, 1)

        return newFile, nil
    }

    // 5. ä¸å­˜åœ¨ï¼Œéœ€è¦æ­£å¸¸ä¸Šä¼ 
    return nil, errors.New(errors.ErrResourceNotFound, "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸Šä¼ ")
}
```

#### âŒ 5. **ä¸Šä¼ è¿›åº¦è·Ÿè¸ª**

**ç°çŠ¶**:
- æ²¡æœ‰è¿›åº¦å›è°ƒæœºåˆ¶
- å‰ç«¯æ— æ³•è·å–å®æ—¶ä¸Šä¼ è¿›åº¦
- ä½¿ç”¨ `io.Copy` æ— æ³•è·Ÿè¸ªè¿›åº¦

**éœ€è¦**:
```go
// è¿›åº¦è·Ÿè¸ªå™¨
type ProgressTracker struct {
    FileID       string
    TotalSize    int64
    UploadedSize int64
    Progress     float64
    Speed        int64  // å­—èŠ‚/ç§’
    StartTime    time.Time
    EstimatedTime int64 // é¢„è®¡å‰©ä½™æ—¶é—´ï¼ˆç§’ï¼‰
}

// å¸¦è¿›åº¦çš„ Reader
type ProgressReader struct {
    reader   io.Reader
    total    int64
    current  int64
    callback func(current, total int64)
}

func (pr *ProgressReader) Read(p []byte) (int, error) {
    n, err := pr.reader.Read(p)
    pr.current += int64(n)

    // å›è°ƒè¿›åº¦
    if pr.callback != nil {
        pr.callback(pr.current, pr.total)
    }

    return n, err
}
```

#### âŒ 6. **OSS åˆ†ç‰‡ä¸Šä¼ æ”¯æŒ**

**ç°çŠ¶**:
```go
// åªä½¿ç”¨äº† PutObjectï¼ˆæ™®é€šä¸Šä¼ ï¼‰
err := s.bucket.PutObject(path, reader, options...)
```

**é˜¿é‡Œäº‘OSS SDK æ”¯æŒä½†æœªä½¿ç”¨**:
```go
// OSS SDK æä¾›çš„åˆ†ç‰‡ä¸Šä¼ APIï¼ˆæœªä½¿ç”¨ï¼‰
- InitiateMultipartUpload()  // åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ 
- UploadPart()               // ä¸Šä¼ åˆ†ç‰‡
- CompleteMultipartUpload()  // å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
- AbortMultipartUpload()     // å–æ¶ˆåˆ†ç‰‡ä¸Šä¼ 
- ListMultipartUploads()     // åˆ—å‡ºæœªå®Œæˆçš„åˆ†ç‰‡ä¸Šä¼ 
- ListUploadedParts()        // åˆ—å‡ºå·²ä¸Šä¼ çš„åˆ†ç‰‡
```

**éœ€è¦å®ç°**:
```go
func (s *AliyunOSS) MultipartUpload(ctx context.Context, path string, reader io.Reader, fileSize int64) (string, error) {
    // 1. åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ 
    imur, err := s.bucket.InitiateMultipartUpload(path)
    if err != nil {
        return "", err
    }

    // 2. åˆ†ç‰‡ä¸Šä¼ 
    chunkSize := int64(5 * 1024 * 1024) // 5MB per chunk
    var parts []oss.UploadPart

    for partNum := 1; ; partNum++ {
        chunk := make([]byte, chunkSize)
        n, err := io.ReadFull(reader, chunk)

        if n > 0 {
            part, err := s.bucket.UploadPart(imur, bytes.NewReader(chunk[:n]), int64(n), partNum)
            if err != nil {
                s.bucket.AbortMultipartUpload(imur) // å–æ¶ˆä¸Šä¼ 
                return "", err
            }
            parts = append(parts, part)
        }

        if err == io.EOF || err == io.ErrUnexpectedEOF {
            break
        }
        if err != nil {
            s.bucket.AbortMultipartUpload(imur)
            return "", err
        }
    }

    // 3. å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
    _, err = s.bucket.CompleteMultipartUpload(imur, parts)
    if err != nil {
        return "", err
    }

    return s.GetURL(ctx, path, 0)
}
```

---

### ä¸‰ã€å¤§æ–‡ä»¶æ”¯æŒåˆ†æ

#### âœ… å·²å®ç°çš„åŠŸèƒ½

1. **æµå¼ä¼ è¾“**:
   ```go
   // ä½¿ç”¨ io.Reader/io.Writer æ¥å£ï¼Œæ”¯æŒæµå¼å¤„ç†
   io.Copy(file, reader)  // ä¸ä¼šä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
   ```

2. **å†…å­˜ç®¡ç†**:
   ```go
   // Gin é…ç½®äº†å†…å­˜ç¼“å­˜é™åˆ¶ï¼ˆcmd/server/main.goï¼‰
   engine.MaxMultipartMemory = 32 << 20 // 32 MB

   // å°æ–‡ä»¶ï¼ˆ< 32MBï¼‰ï¼šå®Œå…¨åœ¨å†…å­˜ä¸­å¤„ç†
   // å¤§æ–‡ä»¶ï¼ˆ> 32MBï¼‰ï¼šè¶…å‡ºéƒ¨åˆ†å†™å…¥ä¸´æ—¶æ–‡ä»¶
   ```

3. **é…é¢æ§åˆ¶**:
   ```go
   // æ”¯æŒæœ€å¤§å•æ–‡ä»¶ 20GB
   MaxFileSize: 20 * 1024 * 1024 * 1024  // 20GB
   ```

#### âš ï¸ å­˜åœ¨çš„é—®é¢˜

1. **ç½‘ç»œè¶…æ—¶**:
   - ä¸Šä¼  20GB æ–‡ä»¶å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´
   - HTTP è¶…æ—¶è®¾ç½®å¯èƒ½ä¸å¤Ÿ
   - éœ€è¦é…ç½®åå‘ä»£ç†è¶…æ—¶ï¼ˆNginx/Apacheï¼‰

2. **å¤±è´¥é‡è¯•**:
   - ä¸Šä¼ å¤±è´¥åå¿…é¡»ä»å¤´å¼€å§‹
   - æµªè´¹å·²ä¼ è¾“çš„æ•°æ®å’Œæ—¶é—´
   - å¯¹äºå¤§æ–‡ä»¶éå¸¸ä¸å‹å¥½

3. **å¹¶å‘æ§åˆ¶**:
   - æ²¡æœ‰ä¸Šä¼ é˜Ÿåˆ—æœºåˆ¶
   - å¤šç”¨æˆ·åŒæ—¶ä¸Šä¼ å¤§æ–‡ä»¶ä¼šæ¶ˆè€—å¤§é‡èµ„æº

4. **ç½‘ç»œç¨³å®šæ€§**:
   - ç½‘ç»œä¸­æ–­ä¼šå¯¼è‡´ä¸Šä¼ å¤±è´¥
   - æ²¡æœ‰è‡ªåŠ¨é‡è¯•æœºåˆ¶

---

### å››ã€æ–‡æ¡£è¯´æ˜

#### 1. `docs/large-file-upload.md`

**å†…å®¹æ‘˜è¦**:
- âœ… è¯´æ˜äº†æ”¯æŒ 20GB å¤§æ–‡ä»¶ä¸Šä¼ 
- âœ… é…ç½®äº† `MaxMultipartMemory = 32MB`
- âœ… ä½¿ç”¨æµå¼ä¸Šä¼ ï¼ˆ`io.Copy`ï¼‰
- âš ï¸ **æ‰¿è®¤äº†é™åˆ¶**ï¼š

```markdown
### 1. åˆ†ç‰‡ä¸Šä¼ 

å¯¹äºè¶…å¤§æ–‡ä»¶ï¼Œå»ºè®®å®ç°åˆ†ç‰‡ä¸Šä¼ ï¼š

ä¼˜ç‚¹ï¼š
- æ”¯æŒæ–­ç‚¹ç»­ä¼         âš ï¸ å»ºè®®å®ç°ä½†æœªå®ç°
- å‡å°‘å•æ¬¡è¯·æ±‚å¤§å°
- æé«˜æˆåŠŸç‡

å®ç°ï¼š
- å‰ç«¯åˆ†ç‰‡ä¸Šä¼         âš ï¸ æœªå®ç°
- åç«¯åˆå¹¶åˆ†ç‰‡        âš ï¸ æœªå®ç°
- å­˜å‚¨åˆ†ç‰‡ä¿¡æ¯        âš ï¸ æœªå®ç°
```

#### 2. `docs/Phase13-äº‘ç›˜åŠŸèƒ½è®¾è®¡æ€»ç»“.md`

**åç»­å·¥ä½œå»ºè®®**:
```markdown
### 5. æ€§èƒ½ä¼˜åŒ–
- ç¼“å­˜æ–‡ä»¶æ ‘ç»“æ„ï¼ˆRedisï¼‰
- å¤§æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼            âš ï¸ æœªå®ç°
- æ–­ç‚¹ç»­ä¼                  âš ï¸ æœªå®ç°
- CDNé¢„çƒ­
- ç¼©ç•¥å›¾å¼‚æ­¥ç”Ÿæˆ
```

---

## ğŸ“Š åŠŸèƒ½å¯¹æ¯”è¡¨

| åŠŸèƒ½ | å½“å‰çŠ¶æ€ | è¯´æ˜ |
|------|---------|------|
| **ä¸Šä¼ åŠŸèƒ½** |
| å•æ¬¡å®Œæ•´ä¸Šä¼  | âœ… æ”¯æŒ | ä½¿ç”¨ `c.FormFile()` å’Œ `io.Copy` |
| æµå¼ä¸Šä¼  | âœ… æ”¯æŒ | ä¸ä¼šä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ |
| åˆ†ç‰‡ä¸Šä¼  | âŒ ä¸æ”¯æŒ | éœ€è¦å®ç° multipart upload |
| æ–­ç‚¹ç»­ä¼  | âŒ ä¸æ”¯æŒ | éœ€è¦ä¸Šä¼ ä¼šè¯ç®¡ç† |
| ç§’ä¼ ï¼ˆMD5å»é‡ï¼‰ | âŒ ä¸æ”¯æŒ | MD5 å­—æ®µæœªä½¿ç”¨ |
| ä¸Šä¼ è¿›åº¦è·Ÿè¸ª | âŒ ä¸æ”¯æŒ | æ²¡æœ‰è¿›åº¦å›è°ƒæœºåˆ¶ |
| å¹¶å‘ä¸Šä¼ æ§åˆ¶ | âŒ ä¸æ”¯æŒ | æ²¡æœ‰é˜Ÿåˆ—æœºåˆ¶ |
| **ä¸‹è½½åŠŸèƒ½** |
| å®Œæ•´ä¸‹è½½ | âœ… æ”¯æŒ | ä½¿ç”¨ `io.Copy` |
| Range è¯·æ±‚ | âŒ ä¸æ”¯æŒ | ä¸æ”¯æŒ HTTP Range å¤´ |
| æ–­ç‚¹ä¸‹è½½ | âŒ ä¸æ”¯æŒ | æ²¡æœ‰ Range æ”¯æŒ |
| **å­˜å‚¨æ”¯æŒ** |
| æœ¬åœ°å­˜å‚¨ | âœ… æ”¯æŒ | å®Œæ•´å®ç° |
| é˜¿é‡Œäº‘ OSS | âš ï¸ éƒ¨åˆ†æ”¯æŒ | ä»…æ”¯æŒæ™®é€šä¸Šä¼ ï¼Œæœªä½¿ç”¨åˆ†ç‰‡API |
| OSS åˆ†ç‰‡ä¸Šä¼  | âŒ ä¸æ”¯æŒ | æœªä½¿ç”¨ SDK çš„ multipart API |
| **é…é¢å’Œé™åˆ¶** |
| å•æ–‡ä»¶å¤§å°é™åˆ¶ | âœ… æ”¯æŒ | æœ€å¤§ 20GB |
| ç”¨æˆ·é…é¢é™åˆ¶ | âœ… æ”¯æŒ | æ€»ç©ºé—´ 10GBï¼ˆå¯é…ç½®ï¼‰ |
| ç½‘ç»œè¶…æ—¶å¤„ç† | âš ï¸ éœ€é…ç½® | éœ€è¦é…ç½®åå‘ä»£ç†è¶…æ—¶ |
| **æ•°æ®ç®¡ç†** |
| æ–‡ä»¶è®°å½•ç®¡ç† | âœ… æ”¯æŒ | å®Œæ•´çš„ CRUD |
| ä¸Šä¼ ä¼šè¯ç®¡ç† | âŒ ä¸æ”¯æŒ | æ²¡æœ‰ä¼šè¯è¡¨ |
| åˆ†ç‰‡è®°å½•ç®¡ç† | âŒ ä¸æ”¯æŒ | æ²¡æœ‰åˆ†ç‰‡è¡¨ |
| MD5 å»é‡ | âŒ ä¸æ”¯æŒ | å­—æ®µå­˜åœ¨ä½†æœªä½¿ç”¨ |

---

## ğŸ¯ æ–­ç‚¹ç»­ä¼ å®ç°å»ºè®®

### æ–¹æ¡ˆä¸€ï¼šåŸºäºæ•°æ®åº“çš„æ–­ç‚¹ç»­ä¼ ï¼ˆæ¨èï¼‰

#### 1. æ•°æ®åº“è®¾è®¡

```sql
-- ä¸Šä¼ ä¼šè¯è¡¨
CREATE TABLE `cloud_upload_session` (
  `ID` BIGINT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
  `FILE_ID` VARCHAR(64) NOT NULL COMMENT 'æ–‡ä»¶å”¯ä¸€æ ‡è¯†ï¼ˆMD5ï¼‰',
  `USER_ID` BIGINT UNSIGNED NOT NULL COMMENT 'ç”¨æˆ·ID',
  `FILE_NAME` VARCHAR(255) NOT NULL COMMENT 'æ–‡ä»¶å',
  `FILE_SIZE` BIGINT NOT NULL COMMENT 'æ–‡ä»¶æ€»å¤§å°',
  `FILE_TYPE` VARCHAR(100) COMMENT 'æ–‡ä»¶ç±»å‹',
  `CHUNK_SIZE` INT NOT NULL DEFAULT 5242880 COMMENT 'åˆ†ç‰‡å¤§å°ï¼ˆé»˜è®¤5MBï¼‰',
  `TOTAL_CHUNKS` INT NOT NULL COMMENT 'æ€»åˆ†ç‰‡æ•°',
  `UPLOADED_CHUNKS` TEXT COMMENT 'å·²ä¸Šä¼ çš„åˆ†ç‰‡ç´¢å¼•ï¼ˆJSONæ•°ç»„ï¼‰',
  `STATUS` VARCHAR(20) NOT NULL DEFAULT 'uploading' COMMENT 'çŠ¶æ€ï¼šuploading,paused,completed,failed',
  `STORAGE_PATH` VARCHAR(500) COMMENT 'ä¸´æ—¶å­˜å‚¨è·¯å¾„',
  `EXPIRE_TIME` TIMESTAMP NOT NULL COMMENT 'è¿‡æœŸæ—¶é—´',
  `CREATE_BY` VARCHAR(50),
  `CREATE_TIME` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  `UPDATE_BY` VARCHAR(50),
  `UPDATE_TIME` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  INDEX `idx_file_id` (`FILE_ID`),
  INDEX `idx_user_id` (`USER_ID`),
  INDEX `idx_status` (`STATUS`),
  INDEX `idx_expire_time` (`EXPIRE_TIME`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='äº‘ç›˜ä¸Šä¼ ä¼šè¯è¡¨';

-- åˆ†ç‰‡è®°å½•è¡¨
CREATE TABLE `cloud_chunk_record` (
  `ID` BIGINT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
  `SESSION_ID` BIGINT UNSIGNED NOT NULL COMMENT 'ä¼šè¯ID',
  `CHUNK_INDEX` INT NOT NULL COMMENT 'åˆ†ç‰‡ç´¢å¼•',
  `CHUNK_SIZE` INT NOT NULL COMMENT 'åˆ†ç‰‡å¤§å°',
  `CHUNK_MD5` VARCHAR(32) NOT NULL COMMENT 'åˆ†ç‰‡MD5',
  `CHUNK_PATH` VARCHAR(500) COMMENT 'åˆ†ç‰‡å­˜å‚¨è·¯å¾„',
  `UPLOADED` TINYINT(1) NOT NULL DEFAULT 0 COMMENT 'æ˜¯å¦å·²ä¸Šä¼ ',
  `UPLOAD_TIME` TIMESTAMP COMMENT 'ä¸Šä¼ æ—¶é—´',
  `CREATE_TIME` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  INDEX `idx_session_id` (`SESSION_ID`),
  INDEX `idx_chunk_index` (`CHUNK_INDEX`),
  UNIQUE KEY `uk_session_chunk` (`SESSION_ID`, `CHUNK_INDEX`),
  FOREIGN KEY (`SESSION_ID`) REFERENCES `cloud_upload_session`(`ID`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='äº‘ç›˜åˆ†ç‰‡è®°å½•è¡¨';
```

#### 2. API è®¾è®¡

```go
// 1. åˆå§‹åŒ–ä¸Šä¼ ä¼šè¯
POST /api/v1/cloud/files/multipart/init
Request:
{
    "fileName": "large_file.mp4",
    "fileSize": 5368709120,      // 5GB
    "fileMD5": "abc123...",
    "chunkSize": 5242880,        // 5MB
    "folderId": 1
}
Response:
{
    "sessionId": "uuid-xxxx",
    "fileId": "abc123...",
    "uploadedChunks": [],         // å·²ä¸Šä¼ çš„åˆ†ç‰‡ï¼ˆæ–­ç‚¹ç»­ä¼ æ—¶è¿”å›ï¼‰
    "uploadUrl": "/api/v1/cloud/files/multipart/upload"
}

// 2. ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
POST /api/v1/cloud/files/multipart/upload
Request (multipart/form-data):
- sessionId: uuid-xxxx
- chunkIndex: 0
- chunkData: binary
- chunkMD5: xyz789...
Response:
{
    "success": true,
    "chunkIndex": 0,
    "uploaded": true
}

// 3. æŸ¥è¯¢ä¸Šä¼ çŠ¶æ€ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
GET /api/v1/cloud/files/multipart/status?sessionId=uuid-xxxx
Response:
{
    "sessionId": "uuid-xxxx",
    "status": "uploading",
    "totalChunks": 1024,
    "uploadedChunks": [0, 1, 2, 5, 6, 7],  // å·²ä¸Šä¼ çš„åˆ†ç‰‡ç´¢å¼•
    "progress": 0.68,                       // ä¸Šä¼ è¿›åº¦
    "expireTime": "2026-01-16T10:00:00Z"
}

// 4. å®Œæˆä¸Šä¼ ï¼ˆåˆå¹¶åˆ†ç‰‡ï¼‰
POST /api/v1/cloud/files/multipart/complete
Request:
{
    "sessionId": "uuid-xxxx",
    "fileMD5": "abc123..."  // ç”¨äºéªŒè¯å®Œæ•´æ€§
}
Response:
{
    "success": true,
    "file": {
        "id": 100,
        "fileName": "large_file.mp4",
        "fileSize": 5368709120,
        "accessUrl": "https://..."
    }
}

// 5. å–æ¶ˆä¸Šä¼ 
DELETE /api/v1/cloud/files/multipart/{sessionId}
Response:
{
    "success": true,
    "message": "ä¸Šä¼ å·²å–æ¶ˆï¼Œä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†"
}
```

#### 3. Service å®ç°

```go
// multipart_upload_service.go

type MultipartUploadService interface {
    // InitUpload åˆå§‹åŒ–ä¸Šä¼ ä¼šè¯
    InitUpload(ctx context.Context, req *InitUploadRequest, userID uint) (*UploadSession, error)

    // UploadChunk ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
    UploadChunk(ctx context.Context, req *UploadChunkRequest, userID uint) error

    // GetUploadStatus è·å–ä¸Šä¼ çŠ¶æ€
    GetUploadStatus(ctx context.Context, sessionID string, userID uint) (*UploadStatus, error)

    // CompleteUpload å®Œæˆä¸Šä¼ ï¼ˆåˆå¹¶åˆ†ç‰‡ï¼‰
    CompleteUpload(ctx context.Context, sessionID string, userID uint) (*entity.CloudFile, error)

    // AbortUpload å–æ¶ˆä¸Šä¼ 
    AbortUpload(ctx context.Context, sessionID string, userID uint) error

    // ResumeUpload æ¢å¤ä¸Šä¼ ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    ResumeUpload(ctx context.Context, fileMD5 string, userID uint) (*UploadSession, error)
}

// InitUpload åˆå§‹åŒ–ä¸Šä¼ ä¼šè¯
func (s *service) InitUpload(ctx context.Context, req *InitUploadRequest, userID uint) (*UploadSession, error) {
    // 1. æ£€æŸ¥é…é¢
    if err := s.CheckQuota(ctx, userID, req.FileSize); err != nil {
        return nil, err
    }

    // 2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªå®Œæˆçš„ä¼šè¯ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    var existingSession entity.UploadSession
    err := s.db.WithContext(ctx).
        Where("FILE_ID = ? AND USER_ID = ? AND STATUS IN (?, ?)",
              req.FileMD5, userID, "uploading", "paused").
        First(&existingSession).Error

    if err == nil {
        // å­˜åœ¨æœªå®Œæˆçš„ä¼šè¯ï¼Œè¿”å›å·²ä¸Šä¼ çš„åˆ†ç‰‡ä¿¡æ¯
        var uploadedChunks []int
        json.Unmarshal([]byte(existingSession.UploadedChunks), &uploadedChunks)

        return &UploadSession{
            SessionID:      existingSession.ID,
            FileID:         existingSession.FileID,
            TotalChunks:    existingSession.TotalChunks,
            UploadedChunks: uploadedChunks,
        }, nil
    }

    // 3. åˆ›å»ºæ–°çš„ä¸Šä¼ ä¼šè¯
    totalChunks := int(math.Ceil(float64(req.FileSize) / float64(req.ChunkSize)))

    session := &entity.UploadSession{
        FileID:         req.FileMD5,
        UserID:         userID,
        FileName:       req.FileName,
        FileSize:       req.FileSize,
        FileType:       req.FileType,
        ChunkSize:      req.ChunkSize,
        TotalChunks:    totalChunks,
        UploadedChunks: "[]",
        Status:         "uploading",
        StoragePath:    fmt.Sprintf("cloud/temp/%d/%s", userID, req.FileMD5),
        ExpireTime:     time.Now().Add(24 * time.Hour), // 24å°æ—¶è¿‡æœŸ
    }

    if err := s.db.WithContext(ctx).Create(session).Error; err != nil {
        return nil, errors.Wrap(errors.ErrDatabase, "åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥", err)
    }

    return &UploadSession{
        SessionID:      session.ID,
        FileID:         session.FileID,
        TotalChunks:    totalChunks,
        UploadedChunks: []int{},
    }, nil
}

// UploadChunk ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
func (s *service) UploadChunk(ctx context.Context, req *UploadChunkRequest, userID uint) error {
    // 1. è·å–ä¸Šä¼ ä¼šè¯
    var session entity.UploadSession
    if err := s.db.WithContext(ctx).
        Where("ID = ? AND USER_ID = ?", req.SessionID, userID).
        First(&session).Error; err != nil {
        return errors.New(errors.ErrResourceNotFound, "ä¸Šä¼ ä¼šè¯ä¸å­˜åœ¨")
    }

    // 2. æ£€æŸ¥ä¼šè¯çŠ¶æ€
    if session.Status != "uploading" && session.Status != "paused" {
        return errors.New(errors.ErrInvalidParam, "ä¸Šä¼ ä¼šè¯çŠ¶æ€æ— æ•ˆ")
    }

    // 3. æ£€æŸ¥ä¼šè¯æ˜¯å¦è¿‡æœŸ
    if session.ExpireTime.Before(time.Now()) {
        return errors.New(errors.ErrInvalidParam, "ä¸Šä¼ ä¼šè¯å·²è¿‡æœŸ")
    }

    // 4. æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²ä¸Šä¼ 
    var chunkRecord entity.ChunkRecord
    err := s.db.WithContext(ctx).
        Where("SESSION_ID = ? AND CHUNK_INDEX = ?", session.ID, req.ChunkIndex).
        First(&chunkRecord).Error

    if err == nil && chunkRecord.Uploaded {
        return nil // åˆ†ç‰‡å·²ä¸Šä¼ ï¼Œè·³è¿‡
    }

    // 5. éªŒè¯åˆ†ç‰‡MD5
    actualMD5 := calculateMD5(req.ChunkData)
    if actualMD5 != req.ChunkMD5 {
        return errors.New(errors.ErrInvalidParam, "åˆ†ç‰‡MD5æ ¡éªŒå¤±è´¥")
    }

    // 6. ä¿å­˜åˆ†ç‰‡åˆ°ä¸´æ—¶ç›®å½•
    chunkPath := fmt.Sprintf("%s/chunk_%d", session.StoragePath, req.ChunkIndex)
    if err := s.storage.Upload(ctx, chunkPath, bytes.NewReader(req.ChunkData), "application/octet-stream"); err != nil {
        return errors.Wrap(errors.ErrInternal, "ä¿å­˜åˆ†ç‰‡å¤±è´¥", err)
    }

    // 7. æ›´æ–°åˆ†ç‰‡è®°å½•
    chunkRecord = entity.ChunkRecord{
        SessionID:  session.ID,
        ChunkIndex: req.ChunkIndex,
        ChunkSize:  len(req.ChunkData),
        ChunkMD5:   req.ChunkMD5,
        ChunkPath:  chunkPath,
        Uploaded:   true,
        UploadTime: time.Now(),
    }

    if err := s.db.WithContext(ctx).Create(&chunkRecord).Error; err != nil {
        return errors.Wrap(errors.ErrDatabase, "åˆ›å»ºåˆ†ç‰‡è®°å½•å¤±è´¥", err)
    }

    // 8. æ›´æ–°ä¸Šä¼ ä¼šè¯çš„å·²ä¸Šä¼ åˆ†ç‰‡åˆ—è¡¨
    var uploadedChunks []int
    json.Unmarshal([]byte(session.UploadedChunks), &uploadedChunks)
    uploadedChunks = append(uploadedChunks, req.ChunkIndex)
    sort.Ints(uploadedChunks)

    uploadedChunksJSON, _ := json.Marshal(uploadedChunks)
    s.db.WithContext(ctx).Model(&entity.UploadSession{}).
        Where("ID = ?", session.ID).
        Update("UPLOADED_CHUNKS", string(uploadedChunksJSON))

    return nil
}

// CompleteUpload å®Œæˆä¸Šä¼ ï¼ˆåˆå¹¶åˆ†ç‰‡ï¼‰
func (s *service) CompleteUpload(ctx context.Context, sessionID string, userID uint) (*entity.CloudFile, error) {
    // 1. è·å–ä¸Šä¼ ä¼šè¯
    var session entity.UploadSession
    if err := s.db.WithContext(ctx).
        Where("ID = ? AND USER_ID = ?", sessionID, userID).
        First(&session).Error; err != nil {
        return nil, errors.New(errors.ErrResourceNotFound, "ä¸Šä¼ ä¼šè¯ä¸å­˜åœ¨")
    }

    // 2. æ£€æŸ¥æ‰€æœ‰åˆ†ç‰‡æ˜¯å¦å·²ä¸Šä¼ 
    var uploadedChunks []int
    json.Unmarshal([]byte(session.UploadedChunks), &uploadedChunks)

    if len(uploadedChunks) != session.TotalChunks {
        return nil, errors.New(errors.ErrInvalidParam,
            fmt.Sprintf("åˆ†ç‰‡æœªå®Œå…¨ä¸Šä¼ ï¼šå·²ä¸Šä¼  %d/%d", len(uploadedChunks), session.TotalChunks))
    }

    // 3. åˆå¹¶åˆ†ç‰‡
    finalPath := fmt.Sprintf("cloud/%d/%s/%s%s",
        userID,
        time.Now().Format("2006/01/02"),
        uuid.New().String(),
        filepath.Ext(session.FileName))

    // åˆ›å»ºæœ€ç»ˆæ–‡ä»¶
    finalFile, err := os.Create(filepath.Join(s.basePath, finalPath))
    if err != nil {
        return nil, errors.Wrap(errors.ErrInternal, "åˆ›å»ºæœ€ç»ˆæ–‡ä»¶å¤±è´¥", err)
    }
    defer finalFile.Close()

    // æŒ‰é¡ºåºåˆå¹¶æ‰€æœ‰åˆ†ç‰‡
    for i := 0; i < session.TotalChunks; i++ {
        chunkPath := fmt.Sprintf("%s/chunk_%d", session.StoragePath, i)

        // è¯»å–åˆ†ç‰‡
        chunkReader, err := s.storage.Download(ctx, chunkPath)
        if err != nil {
            return nil, errors.Wrap(errors.ErrInternal, fmt.Sprintf("è¯»å–åˆ†ç‰‡ %d å¤±è´¥", i), err)
        }

        // å†™å…¥æœ€ç»ˆæ–‡ä»¶
        if _, err := io.Copy(finalFile, chunkReader); err != nil {
            chunkReader.Close()
            return nil, errors.Wrap(errors.ErrInternal, fmt.Sprintf("åˆå¹¶åˆ†ç‰‡ %d å¤±è´¥", i), err)
        }
        chunkReader.Close()
    }

    // 4. éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼ˆMD5ï¼‰
    finalFile.Seek(0, io.SeekStart)
    actualMD5 := calculateFileMD5(finalFile)
    if actualMD5 != session.FileID {
        os.Remove(filepath.Join(s.basePath, finalPath))
        return nil, errors.New(errors.ErrInvalidParam, "æ–‡ä»¶MD5æ ¡éªŒå¤±è´¥")
    }

    // 5. åˆ›å»ºæ–‡ä»¶è®°å½•
    file := &entity.CloudFile{
        FileName:    session.FileName,
        StoragePath: finalPath,
        FileSize:    session.FileSize,
        FileType:    session.FileType,
        MD5:         session.FileID,
        OwnerID:     userID,
        // ...
    }

    if err := s.db.WithContext(ctx).Create(file).Error; err != nil {
        return nil, errors.Wrap(errors.ErrDatabase, "åˆ›å»ºæ–‡ä»¶è®°å½•å¤±è´¥", err)
    }

    // 6. æ›´æ–°é…é¢
    s.UpdateQuota(ctx, userID, session.FileSize, 1)

    // 7. æ›´æ–°ä¼šè¯çŠ¶æ€ä¸ºå·²å®Œæˆ
    s.db.WithContext(ctx).Model(&entity.UploadSession{}).
        Where("ID = ?", session.ID).
        Update("STATUS", "completed")

    // 8. å¼‚æ­¥æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    go s.cleanupChunks(context.Background(), session.ID, session.StoragePath)

    return file, nil
}

// cleanupChunks æ¸…ç†ä¸´æ—¶åˆ†ç‰‡æ–‡ä»¶
func (s *service) cleanupChunks(ctx context.Context, sessionID uint, storagePath string) {
    // 1. åˆ é™¤æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶
    objects, _ := s.storage.ListObjects(ctx, storagePath, 0)
    for _, obj := range objects {
        s.storage.Delete(ctx, obj.Key)
    }

    // 2. åˆ é™¤åˆ†ç‰‡è®°å½•
    s.db.WithContext(ctx).
        Where("SESSION_ID = ?", sessionID).
        Delete(&entity.ChunkRecord{})
}
```

#### 4. å‰ç«¯å®ç°ç¤ºä¾‹

```javascript
// æ–‡ä»¶ä¸Šä¼ ç±»
class ChunkedFileUploader {
    constructor(file, chunkSize = 5 * 1024 * 1024) {
        this.file = file;
        this.chunkSize = chunkSize;
        this.totalChunks = Math.ceil(file.size / chunkSize);
        this.uploadedChunks = [];
        this.sessionId = null;
        this.paused = false;
        this.onProgress = null;
        this.onComplete = null;
        this.onError = null;
    }

    // åˆå§‹åŒ–ä¸Šä¼ 
    async init() {
        // è®¡ç®—æ–‡ä»¶MD5
        const fileMD5 = await this.calculateMD5();

        // åˆå§‹åŒ–ä¸Šä¼ ä¼šè¯
        const response = await fetch('/api/v1/cloud/files/multipart/init', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${token}`
            },
            body: JSON.stringify({
                fileName: this.file.name,
                fileSize: this.file.size,
                fileMD5: fileMD5,
                chunkSize: this.chunkSize,
                folderId: this.folderId
            })
        });

        const data = await response.json();
        this.sessionId = data.sessionId;
        this.uploadedChunks = data.uploadedChunks || [];

        console.log(`åˆå§‹åŒ–æˆåŠŸï¼Œä¼šè¯ID: ${this.sessionId}`);
        console.log(`å·²ä¸Šä¼ åˆ†ç‰‡: ${this.uploadedChunks.length}/${this.totalChunks}`);

        return this.sessionId;
    }

    // å¼€å§‹ä¸Šä¼ 
    async start() {
        if (!this.sessionId) {
            await this.init();
        }

        // ä¸Šä¼ æ‰€æœ‰æœªå®Œæˆçš„åˆ†ç‰‡
        for (let i = 0; i < this.totalChunks; i++) {
            // æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ 
            if (this.uploadedChunks.includes(i)) {
                console.log(`åˆ†ç‰‡ ${i} å·²ä¸Šä¼ ï¼Œè·³è¿‡`);
                continue;
            }

            // æ£€æŸ¥æ˜¯å¦æš‚åœ
            if (this.paused) {
                console.log('ä¸Šä¼ å·²æš‚åœ');
                return;
            }

            try {
                await this.uploadChunk(i);
                this.uploadedChunks.push(i);

                // è§¦å‘è¿›åº¦å›è°ƒ
                const progress = this.uploadedChunks.length / this.totalChunks;
                if (this.onProgress) {
                    this.onProgress(progress, this.uploadedChunks.length, this.totalChunks);
                }
            } catch (error) {
                console.error(`ä¸Šä¼ åˆ†ç‰‡ ${i} å¤±è´¥:`, error);

                // è§¦å‘é”™è¯¯å›è°ƒ
                if (this.onError) {
                    this.onError(error, i);
                }

                // å¯ä»¥é€‰æ‹©é‡è¯•
                // await this.uploadChunk(i);
                return;
            }
        }

        // æ‰€æœ‰åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼Œåˆå¹¶æ–‡ä»¶
        await this.complete();
    }

    // ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
    async uploadChunk(chunkIndex) {
        const start = chunkIndex * this.chunkSize;
        const end = Math.min(start + this.chunkSize, this.file.size);
        const chunk = this.file.slice(start, end);

        // è®¡ç®—åˆ†ç‰‡MD5
        const chunkMD5 = await this.calculateChunkMD5(chunk);

        // æ„é€ FormData
        const formData = new FormData();
        formData.append('sessionId', this.sessionId);
        formData.append('chunkIndex', chunkIndex);
        formData.append('chunkData', chunk);
        formData.append('chunkMD5', chunkMD5);

        // ä¸Šä¼ åˆ†ç‰‡
        const response = await fetch('/api/v1/cloud/files/multipart/upload', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${token}`
            },
            body: formData
        });

        if (!response.ok) {
            throw new Error(`ä¸Šä¼ åˆ†ç‰‡å¤±è´¥: ${response.statusText}`);
        }

        console.log(`åˆ†ç‰‡ ${chunkIndex} ä¸Šä¼ æˆåŠŸ`);
    }

    // å®Œæˆä¸Šä¼ ï¼ˆåˆå¹¶åˆ†ç‰‡ï¼‰
    async complete() {
        const response = await fetch('/api/v1/cloud/files/multipart/complete', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${token}`
            },
            body: JSON.stringify({
                sessionId: this.sessionId
            })
        });

        const data = await response.json();

        console.log('ä¸Šä¼ å®Œæˆ:', data);

        // è§¦å‘å®Œæˆå›è°ƒ
        if (this.onComplete) {
            this.onComplete(data.file);
        }

        return data.file;
    }

    // æš‚åœä¸Šä¼ 
    pause() {
        this.paused = true;
        console.log('ä¸Šä¼ å·²æš‚åœ');
    }

    // æ¢å¤ä¸Šä¼ ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    async resume() {
        // è·å–ä¸Šä¼ çŠ¶æ€
        const response = await fetch(`/api/v1/cloud/files/multipart/status?sessionId=${this.sessionId}`, {
            headers: {
                'Authorization': `Bearer ${token}`
            }
        });

        const data = await response.json();
        this.uploadedChunks = data.uploadedChunks;
        this.paused = false;

        console.log(`æ¢å¤ä¸Šä¼ ï¼Œå·²å®Œæˆ: ${this.uploadedChunks.length}/${this.totalChunks}`);

        // ç»§ç»­ä¸Šä¼ 
        await this.start();
    }

    // å–æ¶ˆä¸Šä¼ 
    async cancel() {
        const response = await fetch(`/api/v1/cloud/files/multipart/${this.sessionId}`, {
            method: 'DELETE',
            headers: {
                'Authorization': `Bearer ${token}`
            }
        });

        console.log('ä¸Šä¼ å·²å–æ¶ˆ');
    }

    // è®¡ç®—æ–‡ä»¶MD5
    async calculateMD5() {
        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onload = (e) => {
                const spark = new SparkMD5.ArrayBuffer();
                spark.append(e.target.result);
                resolve(spark.end());
            };
            reader.onerror = reject;
            reader.readAsArrayBuffer(this.file);
        });
    }

    // è®¡ç®—åˆ†ç‰‡MD5
    async calculateChunkMD5(chunk) {
        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onload = (e) => {
                const spark = new SparkMD5.ArrayBuffer();
                spark.append(e.target.result);
                resolve(spark.end());
            };
            reader.onerror = reject;
            reader.readAsArrayBuffer(chunk);
        });
    }
}

// ä½¿ç”¨ç¤ºä¾‹
async function uploadFile(file) {
    const uploader = new ChunkedFileUploader(file);

    // è®¾ç½®è¿›åº¦å›è°ƒ
    uploader.onProgress = (progress, uploaded, total) => {
        console.log(`ä¸Šä¼ è¿›åº¦: ${(progress * 100).toFixed(2)}%`);
        console.log(`å·²ä¸Šä¼ åˆ†ç‰‡: ${uploaded}/${total}`);

        // æ›´æ–°UI
        updateProgressBar(progress);
    };

    // è®¾ç½®å®Œæˆå›è°ƒ
    uploader.onComplete = (file) => {
        console.log('æ–‡ä»¶ä¸Šä¼ å®Œæˆ:', file);
        alert('ä¸Šä¼ æˆåŠŸï¼');
    };

    // è®¾ç½®é”™è¯¯å›è°ƒ
    uploader.onError = (error, chunkIndex) => {
        console.error(`åˆ†ç‰‡ ${chunkIndex} ä¸Šä¼ å¤±è´¥:`, error);

        // å¯ä»¥é€‰æ‹©æš‚åœä¸Šä¼ ï¼Œç­‰å¾…ç”¨æˆ·é‡è¯•
        uploader.pause();

        if (confirm('ä¸Šä¼ å¤±è´¥ï¼Œæ˜¯å¦é‡è¯•ï¼Ÿ')) {
            uploader.resume(); // æ–­ç‚¹ç»­ä¼ 
        }
    };

    // å¼€å§‹ä¸Šä¼ 
    await uploader.start();
}

// é¡µé¢åˆ·æ–°æ—¶ä¿å­˜ä¸Šä¼ çŠ¶æ€
window.addEventListener('beforeunload', (e) => {
    if (uploader && !uploader.paused && uploader.uploadedChunks.length < uploader.totalChunks) {
        // ä¿å­˜ä¼šè¯IDåˆ° localStorage
        localStorage.setItem('uploadSessionId', uploader.sessionId);

        e.preventDefault();
        e.returnValue = 'æ–‡ä»¶æ­£åœ¨ä¸Šä¼ ï¼Œç¡®å®šè¦ç¦»å¼€å—ï¼Ÿ';
    }
});

// é¡µé¢åŠ è½½æ—¶æ¢å¤ä¸Šä¼ 
window.addEventListener('load', async () => {
    const sessionId = localStorage.getItem('uploadSessionId');
    if (sessionId) {
        if (confirm('æ£€æµ‹åˆ°æœªå®Œæˆçš„ä¸Šä¼ ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ')) {
            const uploader = new ChunkedFileUploader(file);
            uploader.sessionId = sessionId;
            await uploader.resume(); // æ–­ç‚¹ç»­ä¼ 

            localStorage.removeItem('uploadSessionId');
        }
    }
});
```

---

### æ–¹æ¡ˆäºŒï¼šåŸºäº OSS çš„æ–­ç‚¹ç»­ä¼ ï¼ˆé€‚åˆäº‘å­˜å‚¨ï¼‰

å¦‚æœä½¿ç”¨é˜¿é‡Œäº‘ OSSï¼Œå¯ä»¥åˆ©ç”¨ OSS SDK çš„åˆ†ç‰‡ä¸Šä¼ åŠŸèƒ½ï¼š

```go
// OSS åˆ†ç‰‡ä¸Šä¼ å®ç°
func (s *AliyunOSS) MultipartUpload(ctx context.Context, path string, reader io.Reader, fileSize int64) (string, error) {
    // 1. åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ 
    imur, err := s.bucket.InitiateMultipartUpload(path)
    if err != nil {
        return "", errors.Wrap(errors.ErrInternal, "åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ å¤±è´¥", err)
    }

    // 2. åˆ†ç‰‡ä¸Šä¼ 
    chunkSize := int64(5 * 1024 * 1024) // 5MB per chunk
    var parts []oss.UploadPart
    buffer := make([]byte, chunkSize)

    for partNum := 1; ; partNum++ {
        // è¯»å–åˆ†ç‰‡
        n, err := io.ReadFull(reader, buffer)

        if n > 0 {
            // ä¸Šä¼ åˆ†ç‰‡
            part, uploadErr := s.bucket.UploadPart(
                imur,
                bytes.NewReader(buffer[:n]),
                int64(n),
                partNum,
            )

            if uploadErr != nil {
                // ä¸Šä¼ å¤±è´¥ï¼Œå–æ¶ˆåˆ†ç‰‡ä¸Šä¼ 
                s.bucket.AbortMultipartUpload(imur)
                return "", errors.Wrap(errors.ErrInternal, "ä¸Šä¼ åˆ†ç‰‡å¤±è´¥", uploadErr)
            }

            parts = append(parts, part)
        }

        // æ£€æŸ¥æ˜¯å¦è¯»å–å®Œæ¯•
        if err == io.EOF || err == io.ErrUnexpectedEOF {
            break
        }

        if err != nil {
            s.bucket.AbortMultipartUpload(imur)
            return "", errors.Wrap(errors.ErrInternal, "è¯»å–æ–‡ä»¶å¤±è´¥", err)
        }
    }

    // 3. å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
    _, err = s.bucket.CompleteMultipartUpload(imur, parts)
    if err != nil {
        return "", errors.Wrap(errors.ErrInternal, "å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¤±è´¥", err)
    }

    // 4. è¿”å›æ–‡ä»¶URL
    return s.GetURL(ctx, path, 0)
}

// åˆ—å‡ºæœªå®Œæˆçš„åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡ï¼ˆç”¨äºæ¢å¤ä¸Šä¼ ï¼‰
func (s *AliyunOSS) ListMultipartUploads(ctx context.Context, prefix string) ([]oss.UncompletedUpload, error) {
    lmu, err := s.bucket.ListMultipartUploads(oss.Prefix(prefix))
    if err != nil {
        return nil, errors.Wrap(errors.ErrInternal, "åˆ—ä¸¾æœªå®Œæˆçš„åˆ†ç‰‡ä¸Šä¼ å¤±è´¥", err)
    }
    return lmu.Uploads, nil
}

// åˆ—å‡ºå·²ä¸Šä¼ çš„åˆ†ç‰‡ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
func (s *AliyunOSS) ListUploadedParts(ctx context.Context, uploadID string, objectKey string) ([]oss.UploadPart, error) {
    imur := oss.InitiateMultipartUploadResult{
        UploadID: uploadID,
        Key:      objectKey,
        Bucket:   s.bucketName,
    }

    lp, err := s.bucket.ListUploadedParts(imur)
    if err != nil {
        return nil, errors.Wrap(errors.ErrInternal, "åˆ—ä¸¾å·²ä¸Šä¼ çš„åˆ†ç‰‡å¤±è´¥", err)
    }

    return lp.UploadedParts, nil
}

// æ¢å¤åˆ†ç‰‡ä¸Šä¼ ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
func (s *AliyunOSS) ResumeMultipartUpload(ctx context.Context, uploadID string, objectKey string, reader io.Reader, fileSize int64) (string, error) {
    // 1. è·å–å·²ä¸Šä¼ çš„åˆ†ç‰‡
    uploadedParts, err := s.ListUploadedParts(ctx, uploadID, objectKey)
    if err != nil {
        return "", err
    }

    // 2. è®¡ç®—å·²ä¸Šä¼ çš„å­—èŠ‚æ•°
    uploadedSize := int64(0)
    for _, part := range uploadedParts {
        uploadedSize += part.Size
    }

    // 3. è·³è¿‡å·²ä¸Šä¼ çš„éƒ¨åˆ†
    if seeker, ok := reader.(io.Seeker); ok {
        _, err := seeker.Seek(uploadedSize, io.SeekStart)
        if err != nil {
            return "", errors.Wrap(errors.ErrInternal, "å®šä½æ–‡ä»¶å¤±è´¥", err)
        }
    }

    // 4. ç»§ç»­ä¸Šä¼ å‰©ä½™åˆ†ç‰‡
    imur := oss.InitiateMultipartUploadResult{
        UploadID: uploadID,
        Key:      objectKey,
        Bucket:   s.bucketName,
    }

    chunkSize := int64(5 * 1024 * 1024)
    buffer := make([]byte, chunkSize)
    partNum := len(uploadedParts) + 1

    for {
        n, err := io.ReadFull(reader, buffer)

        if n > 0 {
            part, uploadErr := s.bucket.UploadPart(
                imur,
                bytes.NewReader(buffer[:n]),
                int64(n),
                partNum,
            )

            if uploadErr != nil {
                return "", errors.Wrap(errors.ErrInternal, "ä¸Šä¼ åˆ†ç‰‡å¤±è´¥", uploadErr)
            }

            uploadedParts = append(uploadedParts, part)
            partNum++
        }

        if err == io.EOF || err == io.ErrUnexpectedEOF {
            break
        }

        if err != nil {
            return "", errors.Wrap(errors.ErrInternal, "è¯»å–æ–‡ä»¶å¤±è´¥", err)
        }
    }

    // 5. å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
    _, err = s.bucket.CompleteMultipartUpload(imur, uploadedParts)
    if err != nil {
        return "", errors.Wrap(errors.ErrInternal, "å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¤±è´¥", err)
    }

    return s.GetURL(ctx, objectKey, 0)
}
```

---

## ğŸ“ æ€»ç»“

### å½“å‰çŠ¶æ€

| é¡¹ç›® | çŠ¶æ€ |
|------|------|
| **ä¸Šä¼ ** | âœ… æ”¯æŒå•æ¬¡å®Œæ•´ä¸Šä¼ ï¼ˆæœ€å¤§20GBï¼‰ |
| | âœ… æ”¯æŒæµå¼å¤„ç†ï¼ˆä¸ä¼šä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ï¼‰ |
| | âŒ ä¸æ”¯æŒåˆ†ç‰‡ä¸Šä¼  |
| | âŒ ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼  |
| | âŒ ä¸æ”¯æŒä¸Šä¼ è¿›åº¦è·Ÿè¸ª |
| **ä¸‹è½½** | âœ… æ”¯æŒå®Œæ•´ä¸‹è½½ |
| | âŒ ä¸æ”¯æŒ HTTP Range è¯·æ±‚ |
| | âŒ ä¸æ”¯æŒæ–­ç‚¹ä¸‹è½½ |
| **å­˜å‚¨** | âœ… æœ¬åœ°å­˜å‚¨å®Œæ•´å®ç° |
| | âš ï¸ OSS ä»…ä½¿ç”¨æ™®é€šä¸Šä¼ ï¼Œæœªä½¿ç”¨åˆ†ç‰‡API |

### å®ç°å»ºè®®

**ä¼˜å…ˆçº§æ’åº**:

1. **é«˜ä¼˜å…ˆçº§**ï¼ˆæ¨èå…ˆå®ç°ï¼‰:
   - âœ… åˆ†ç‰‡ä¸Šä¼ åŸºç¡€åŠŸèƒ½
   - âœ… æ–­ç‚¹ç»­ä¼ ï¼ˆåŸºäºä¼šè¯ç®¡ç†ï¼‰
   - âœ… ä¸Šä¼ è¿›åº¦è·Ÿè¸ª
   - âœ… MD5 å»é‡ï¼ˆç§’ä¼ ï¼‰

2. **ä¸­ä¼˜å…ˆçº§**:
   - âœ… HTTP Range è¯·æ±‚æ”¯æŒ
   - âœ… OSS åˆ†ç‰‡ä¸Šä¼ é›†æˆ
   - âœ… å¹¶å‘ä¸Šä¼ æ§åˆ¶

3. **ä½ä¼˜å…ˆçº§**:
   - âœ… è‡ªåŠ¨é‡è¯•æœºåˆ¶
   - âœ… ä¸Šä¼ é˜Ÿåˆ—ç®¡ç†
   - âœ… è¿‡æœŸä¼šè¯æ¸…ç†

### å·¥ä½œé‡è¯„ä¼°

| ä»»åŠ¡ | å·¥ä½œé‡ | è¯´æ˜ |
|------|--------|------|
| æ•°æ®åº“è®¾è®¡ | 0.5å¤© | 2ä¸ªæ–°è¡¨ |
| åç«¯APIå®ç° | 2-3å¤© | 5ä¸ªæ–°æ¥å£ + Service |
| å­˜å‚¨å±‚æ”¹é€  | 1-2å¤© | æ”¯æŒåˆ†ç‰‡ä¸Šä¼  |
| å‰ç«¯å®ç° | 2-3å¤© | åˆ†ç‰‡ä¸Šä¼  + æ–­ç‚¹ç»­ä¼ UI |
| æµ‹è¯•è°ƒè¯• | 1-2å¤© | åŠŸèƒ½æµ‹è¯• + è¾¹ç•Œæµ‹è¯• |
| **æ€»è®¡** | **7-11å¤©** | 1-2å‘¨ |

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- `docs/large-file-upload.md` - å¤§æ–‡ä»¶ä¸Šä¼ è¯´æ˜
- `docs/Phase13-äº‘ç›˜åŠŸèƒ½è®¾è®¡æ€»ç»“.md` - äº‘ç›˜åŠŸèƒ½æ€»ç»“
- `internal/service/cloud/cloud_service.go` - äº‘ç›˜æœåŠ¡å®ç°
- `internal/pkg/storage/` - å­˜å‚¨å±‚å®ç°
- é˜¿é‡Œäº‘OSS SDKæ–‡æ¡£: https://help.aliyun.com/document_detail/32144.html

---

**åˆ†æå®Œæˆæ—¶é—´**: 2026-01-15
**åˆ†æäººå‘˜**: Claude Code Assistant
**ç»“è®º**: âŒ å½“å‰ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œä½†æä¾›äº†å®Œæ•´çš„å®ç°æ–¹æ¡ˆ
